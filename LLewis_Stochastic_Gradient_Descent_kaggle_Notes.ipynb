{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1IszV-YDaF6AlG7ooHZrMdEi6Z1MlfWqC",
      "authorship_tag": "ABX9TyMyMK98icvRbUdcuqrubCtH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LLewis/kaggle-deep-learning/blob/main/LLewis_Stochastic_Gradient_Descent_kaggle_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stochastic Gradient Descdent Notes\n",
        "\n",
        "Training the neural network\n",
        "- reference:  https://www.kaggle.com/code/ryanholbrook/stochastic-gradient-descent\n",
        "\n",
        "- each training data example consist of:\n",
        " - some features (inputs)\n",
        " - with an expected target (the output)\n",
        "Training a network means adjusting its weights in such a way that\n",
        "it can transform the features into the target.\n",
        "\n",
        "Great explaination:\n",
        "- reference:  https://www.kaggle.com/code/ryanholbrook/stochastic-gradient-descent\n",
        "- In the 80 Cereals dataset, for instance, we want a network that can take each cereal's 'sugar', 'fiber', and 'protein' content and produce a prediction for that cereal's 'calories'. If we can successfully train a network to do that, its weights must represent in some way the relationship between those features and that target as expressed in the training data.\n",
        "\n",
        "In addition to the training data, we need two more things:\n",
        "\n",
        "    A \"loss function\" that measures how good the network's predictions are.\n",
        "    An \"optimizer\" that can tell the network how to change its weights.\n"
      ],
      "metadata": {
        "id": "bDQ7p4EX0Ebr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function - in addition to designing an architecture for a network, I must tell a network what problem to solve.  This is the job of the loss func.\n",
        "\n",
        "- reference:  https://www.kaggle.com/code/ryanholbrook/stochastic-gradient-descent\n",
        "\n",
        "- Loss function - measures the disparity between the target's true value and the value the model predicted.\n",
        "\n",
        "- Regression problems, where the task is to predict some numeric value\n",
        "- examples: calories in 70 cereal's, the rating in white wine quality, predicting the price of a house, predicting the fuel effiecieny of a car\n",
        "\n",
        "- Common loss function for regression problems is the mean absolute error MAE\n",
        "\n",
        "- so MAE - for each prediction y_pred, MAE measures the disparity from the true target y_true by an absolute difference.\n",
        "\n",
        "- abs(y_true - y_pred)\n",
        "\n",
        "-  so the model uses the loss func as a guide for finding the correct values\n",
        "of its weights  - lower loss is better - loss func provides the network with an objective\n",
        "\n",
        "-other loss func for regression\n",
        "   - mean-squared error (MSE)\n",
        "   - Huber loss func"
      ],
      "metadata": {
        "id": "DwcL0JZv2xb7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimizer - Stochastic Gradient Descent - SGD\n",
        "optimizer is an algorithm that adjusts the weights to minimize the loss.\n",
        "\n",
        "- Each iteration's sample of training data is called a minibatch (or often just \"batch\"), while a complete round of the training data is called an epoch. The number of epochs you train for is how many times the network will see each training example.\n",
        "\n",
        "- Learning Rate and Batch Size\n",
        "  - The size of these shifts is determined by the learning rate. A smaller learning rate means the network needs to see more minibatches before its weights converge to their best values.\n",
        "\n",
        "  - The learning rate and the size of the minibatches are the two parameters that have the largest effect on how the SGD training proceeds. Their interaction is often subtle and the right choice for these parameters isn't always obvious.\n",
        "\n",
        "  - using Adam - an SGD algorithm that has an adaptive learning rate, great general purpose optimizer\n",
        "\n",
        "- The gradient is a vector that tells us in what direction the weights need to go. More precisely, it tells us how to change the weights to make the loss change fastest. We call our process gradient descent because it uses the gradient to descend the loss curve towards a minimum. Stochastic means \"determined by chance.\" Our training is stochastic because the minibatches are random samples from the dataset. And that's why it's called SGD\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_rshrii87dUf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsxpP20bz1HU"
      },
      "outputs": [],
      "source": []
    }
  ]
}